---
description: https://www.youtube.com/watch?v=bCz4OMemCcA
---

# Attention is All Your Need

Recurrent Neural Networks (RNN) is sequential process.

* Slow computation for long sequences
*   Vanishing or exploding gradient

    d(`output`)/d(`input`) is a chain of  d(`next_state_function`)/d(`current_state_function`), so it may be too small if each gradient is < 1 or too large vice versa.&#x20;
*

